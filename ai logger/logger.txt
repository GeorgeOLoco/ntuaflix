User
i will be giving you 7 .tsv files and i want you to write a python script that reads ALL the lines form all the files and creates a .sql file that has all the input for a database,

the table of the database is the following:

DROP DATABASE IF EXISTS ntuaflix;

CREATE DATABASE ntuaflix;
USE ntuaflix;

CREATE TABLE users (
    userID INT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(255),
    email VARCHAR(255),
    password_hashed VARCHAR(255)
);

CREATE TABLE people (
    nconst VARCHAR(255) PRIMARY KEY,
    primaryName VARCHAR(255),
    birthYear VARCHAR(255),
    deathYear VARCHAR(255),
    primaryProfession VARCHAR(255),
    knownForTitles VARCHAR(255),
    img_url_asset VARCHAR(255)
);

CREATE TABLE Titles (
    tconst VARCHAR(255) PRIMARY KEY,
    titletype VARCHAR(255),
    primarytitle VARCHAR(255),
    originaltitle VARCHAR(255),
    isAdult BOOLEAN,
    startYear VARCHAR(255),
    endYear VARCHAR(255),
    runtimeMinutes VARCHAR(255),
    genres VARCHAR(255),
    img_url_asset VARCHAR(255)
);

CREATE TABLE Episodes (
    episodeID INT AUTO_INCREMENT PRIMARY KEY,
    seriesID VARCHAR(255),
    seasonN VARCHAR(255),
    episodeN VARCHAR(255),
    FOREIGN KEY (seriesID) REFERENCES Titles(tconst)
);

CREATE TABLE Title_ratings (
    ratingID INT AUTO_INCREMENT PRIMARY KEY,
    titleid VARCHAR(255),
    averageRate DECIMAL(3, 2),
    numVotes INT,
    FOREIGN KEY (titleid) REFERENCES Titles(tconst)
);

CREATE TABLE title_crew (
    tconst VARCHAR(255),
    directors VARCHAR(255),
    writers VARCHAR(255),
    PRIMARY KEY (tconst),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE title_principals (
    tconst VARCHAR(255),
    ordering INT,
    nconst VARCHAR(255),
    category VARCHAR(255),
    job VARCHAR(255),
    characters VARCHAR(255),
    img_url_assets VARCHAR(255),
    PRIMARY KEY (tconst, ordering),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst),
    FOREIGN KEY (nconst) REFERENCES people(nconst)
);

CREATE TABLE title_akas (
    tconst VARCHAR(255),
    ordering INT,
    title VARCHAR(255),
    region VARCHAR(255),
    language VARCHAR(255),
    types VARCHAR(255),
    attributes VARCHAR(255),
    isOriginalTitle BOOLEAN,
    PRIMARY KEY (tconst, ordering),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE watchlist (
    userID INT,
    tconst VARCHAR(255),
    watched BOOLEAN,
    PRIMARY KEY (userID, tconst),
    FOREIGN KEY (userID) REFERENCES users(userID),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE forum (
    forumID INT AUTO_INCREMENT PRIMARY KEY,
    tconst VARCHAR(255),
    title VARCHAR(255),
    description VARCHAR(255),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE comments (
    commentID INT AUTO_INCREMENT PRIMARY KEY,
    forumID INT,
    userID INT,
    commentText VARCHAR(255),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (forumID) REFERENCES forum(forumID),
    FOREIGN KEY (userID) REFERENCES users(userID)
);


and i will be giving you the title of each .tsv, each ones, the first line which consists the columns and then one row as an example:


truncated_name.basics.tsv:
nconst	primaryName	birthYear	deathYear	primaryProfession	knownForTitles	img_url_asset
nm0000019	Federico Fellini	1920	1993	writer,director,actor	tt0071129,tt0047528,tt0050783,tt0056801	https://image.tmdb.org/t/p/{width_variable}/jH2VnHAuI0UbTWsnrjMPro0fC9j.jpg

truncated_title.akas.tsv:
titleId	ordering	title	region	language	types	attributes	isOriginalTitle
tt0000929	1	Willys Streiche: Klebolin klebt alles	DE	\N	alternative	\N	0

truncated_title.basics.tsv:
tconst	titleType	primaryTitle	originalTitle	isAdult	startYear	endYear	runtimeMinutes	genres	img_url_asset
tt0000929	short	Klebolin klebt alles	Klebolin klebt alles	0	1990	\N	\N	Comedy,Short	\N

truncated_title.crew.tsv:
tconst	directors	writers
tt0088656	nm0632726,nm0407479	nm0014556,nm0840697,nm0847638

truncated_title.episode.tsv:
tconst	parentTconst	seasonNumber	episodeNumber
tt0073481	tt0298685	1	4

truncated_title.principals.tsv:
tconst	ordering	nconst	category	job	characters	img_url_asset
tt0000929	1	nm0066941	actor	\N	\N	\N

truncated_title.ratings.tsv:
tconst	averageRating	numVotes
tt0000929	5.3	46


ask any questions that you may have so that i can help you understand what i need

ChatGPT
To ensure I understand your requirements correctly and create an appropriate Python script, I have a few clarifications and questions:

File Format and Structure: Are all the .tsv files structured in the same way, with the first line as the header and the subsequent lines as data entries?

Data Types and Constraints: For each column in the .tsv files, should I strictly adhere to the data types and constraints defined in your SQL table schema? For example, handling missing values (\N) or converting boolean values (like isAdult and isOriginalTitle) to the appropriate format for SQL.

Database Insert Statements: Do you require the Python script to generate SQL INSERT statements for each row in the .tsv files, correctly formatted to match the schema of the respective tables?

Handling Large Data: If these .tsv files are large, would you prefer a method that handles the data in chunks to avoid memory issues, or is this not a concern?

File Handling: Should the script directly generate a .sql file containing all the SQL insert commands, or would you prefer the script to display the commands on the console for manual copying?

Error Handling: How should the script handle potential errors, such as data format mismatches or missing values in essential fields?

Answering these questions will enable me to create a more accurate and efficient script for your needs.




User
1) yes
2) if its \N dont insert them into the database, if they are null they are null. and yes handle the booleans as booleans, since even in the database they are boolean, have 0 be false and 1 be true obviously
3) yes please
4) not really a concern
5) create the .sql file so that it will be ready
6) if there are missing values just insert null at that position, if there are mismatches print it on the screen

ChatGPT
Thank you for the clarifications. Based on your requirements, I'll write a Python script that:

Reads each .tsv file.
Skips rows where key fields (like tconst or nconst) are marked as \N.
Converts boolean values (0 to FALSE, 1 to TRUE).
Generates INSERT statements for each row, inserting NULL for any missing or \N values.
Writes these statements to a .sql file.
Reports any format mismatches or other issues to the screen.
Here's the script that fulfills these requirements:

python
Copy code
import csv
import re

# Define the mapping of TSV files to their corresponding table names and columns
file_table_mappings = {
    'truncated_name.basics.tsv': ('people', ['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles', 'img_url_asset']),
    'truncated_title.akas.tsv': ('title_akas', ['titleId', 'ordering', 'title', 'region', 'language', 'types', 'attributes', 'isOriginalTitle']),
    'truncated_title.basics.tsv': ('Titles', ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres', 'img_url_asset']),
    'truncated_title.crew.tsv': ('title_crew', ['tconst', 'directors', 'writers']),
    'truncated_title.episode.tsv': ('Episodes', ['tconst', 'parentTconst', 'seasonNumber', 'episodeNumber']),
    'truncated_title.principals.tsv': ('title_principals', ['tconst', 'ordering', 'nconst', 'category', 'job', 'characters', 'img_url_asset']),
    'truncated_title.ratings.tsv': ('Title_ratings', ['tconst', 'averageRating', 'numVotes'])
}

# Function to create SQL INSERT statements from TSV data
def create_insert_statements(filename, table, columns):
    insert_statements = []
    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file, delimiter='\t', fieldnames=columns)

        next(reader)  # Skip header row

        for row in reader:
            # Skip rows with missing key fields
            if '\N' in [row[col] for col in ['tconst', 'nconst'] if col in row]:
                continue

            # Prepare values, replace '\N' with NULL and handle booleans
            values = [f"'{re.sub(r'\'', r'\'\'', row[col])}'" if row[col] != '\\N' else 'NULL' for col in columns]
            values = [f"TRUE" if value == "'1'" else f"FALSE" if value == "'0'" else value for value in values]

            # Create and append the INSERT statement
            insert_statement = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(values)});"
            insert_statements.append(insert_statement)

    return insert_statements

# Main process
sql_statements = []
for filename, (table, columns) in file_table_mappings.items():
    try:
        sql_statements += create_insert_statements(filename, table, columns)
    except Exception as e:
        print(f"Error processing {filename}: {e}")

# Write the SQL statements to a file
with open('database_inserts.sql', 'w', encoding='utf-8') as sql_file:
    sql_file.write('\n'.join(sql_statements))

print("SQL file generated successfully.")
To use this script:

Ensure the .tsv files are in the same directory as the script, or modify the file paths in the file_table_mappings dictionary accordingly.
Run the script. It will generate a database_inserts.sql file with all the INSERT statements.
Check the console for any error messages during processing.
Remember, this script assumes the structure of the .tsv files matches the given database schema. If there are discrepancies, you might need to adjust the script accordingly.




User
im getting this error:

  File "C:\Users\chris\OneDrive\Desktop\truncated_data\run.py", line 25
    if '\N' in [row[col] for col in ['tconst', 'nconst'] if col in row]:
            ^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 0-1: malformed \N character escape

ChatGPT
The error you're encountering is due to the way Python interprets string literals. In Python, \N is interpreted as a special escape sequence in a string, but it seems like it's not recognized in your context. To fix this, you can use a raw string (prefixed with r) for the \N part. Let's adjust the script accordingly:

python
Copy code
import csv
import re

# Define the mapping of TSV files to their corresponding table names and columns
file_table_mappings = {
    'truncated_name.basics.tsv': ('people', ['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles', 'img_url_asset']),
    'truncated_title.akas.tsv': ('title_akas', ['titleId', 'ordering', 'title', 'region', 'language', 'types', 'attributes', 'isOriginalTitle']),
    'truncated_title.basics.tsv': ('Titles', ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres', 'img_url_asset']),
    'truncated_title.crew.tsv': ('title_crew', ['tconst', 'directors', 'writers']),
    'truncated_title.episode.tsv': ('Episodes', ['tconst', 'parentTconst', 'seasonNumber', 'episodeNumber']),
    'truncated_title.principals.tsv': ('title_principals', ['tconst', 'ordering', 'nconst', 'category', 'job', 'characters', 'img_url_asset']),
    'truncated_title.ratings.tsv': ('Title_ratings', ['tconst', 'averageRating', 'numVotes'])
}

# Function to create SQL INSERT statements from TSV data
def create_insert_statements(filename, table, columns):
    insert_statements = []
    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file, delimiter='\t', fieldnames=columns)

        next(reader)  # Skip header row

        for row in reader:
            # Skip rows with missing key fields
            if r'\N' in [row[col] for col in ['tconst', 'nconst'] if col in row]:
                continue

            # Prepare values, replace '\N' with NULL and handle booleans
            values = [f"'{re.sub(r'\'', r'\'\'', row[col])}'" if row[col] != r'\N' else 'NULL' for col in columns]
            values = [f"TRUE" if value == "'1'" else f"FALSE" if value == "'0'" else value for value in values]

            # Create and append the INSERT statement
            insert_statement = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(values)});"
            insert_statements.append(insert_statement)

    return insert_statements

# Main process
sql_statements = []
for filename, (table, columns) in file_table_mappings.items():
    try:
        sql_statements += create_insert_statements(filename, table, columns)
    except Exception as e:
        print(f"Error processing {filename}: {e}")

# Write the SQL statements to a file
with open('database_inserts.sql', 'w', encoding='utf-8') as sql_file:
    sql_file.write('\n'.join(sql_statements))

print("SQL file generated successfully.")
This change uses r'\N' instead of '\N', treating the string as a raw string and preventing Python from interpreting \N as an escape sequence. Try running this adjusted script, and it should resolve the issue you encountered.




User
im still getting an error, look at this:

C:\Users\chris\OneDrive\Desktop\truncated_data>python3 run.py
  File "C:\Users\chris\OneDrive\Desktop\truncated_data\run.py", line 29
    values = [f"'{re.sub(r'\'', r'\'\'', row[col])}'" if row[col] != r'\N' else 'NULL' for col in columns]

ChatGPT
The error you're encountering now seems to be related to how string literals and escape sequences are being handled within list comprehensions and f-strings. Let's adjust the script to handle these more robustly.

We'll use a helper function to process each value, which will simplify the comprehension and make the escape handling clearer:

python
Copy code
import csv
import re

# Define the mapping of TSV files to their corresponding table names and columns
file_table_mappings = {
    'truncated_name.basics.tsv': ('people', ['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles', 'img_url_asset']),
    'truncated_title.akas.tsv': ('title_akas', ['titleId', 'ordering', 'title', 'region', 'language', 'types', 'attributes', 'isOriginalTitle']),
    'truncated_title.basics.tsv': ('Titles', ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres', 'img_url_asset']),
    'truncated_title.crew.tsv': ('title_crew', ['tconst', 'directors', 'writers']),
    'truncated_title.episode.tsv': ('Episodes', ['tconst', 'parentTconst', 'seasonNumber', 'episodeNumber']),
    'truncated_title.principals.tsv': ('title_principals', ['tconst', 'ordering', 'nconst', 'category', 'job', 'characters', 'img_url_asset']),
    'truncated_title.ratings.tsv': ('Title_ratings', ['tconst', 'averageRating', 'numVotes'])
}

# Helper function to process each value
def process_value(value):
    if value == r'\N':
        return 'NULL'
    if value == '1':
        return 'TRUE'
    if value == '0':
        return 'FALSE'
    # Escape single quotes for SQL
    return f"'{re.sub(r'\'', r'\'\'', value)}'"

# Function to create SQL INSERT statements from TSV data
def create_insert_statements(filename, table, columns):
    insert_statements = []
    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file, delimiter='\t', fieldnames=columns)

        next(reader)  # Skip header row

        for row in reader:
            # Skip rows with missing key fields
            if r'\N' in [row[col] for col in ['tconst', 'nconst'] if col in row]:
                continue

            # Prepare values and create the INSERT statement
            values = [process_value(row[col]) for col in columns]
            insert_statement = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(values)});"
            insert_statements.append(insert_statement)

    return insert_statements

# Main process
sql_statements = []
for filename, (table, columns) in file_table_mappings.items():
    try:
        sql_statements += create_insert_statements(filename, table, columns)
    except Exception as e:
        print(f"Error processing {filename}: {e}")

# Write the SQL statements to a file
with open('database_inserts.sql', 'w', encoding='utf-8') as sql_file:
    sql_file.write('\n'.join(sql_statements))

print("SQL file generated successfully.")
In this version, the process_value function takes care of escaping single quotes and converting special values (\N, 1, 0) appropriately. This should resolve the syntax error you are experiencing. Please try running this adjusted script.




User
start improving the code...
look at the error now:

C:\Users\chris\OneDrive\Desktop\truncated_data>python3 run.py
  File "C:\Users\chris\OneDrive\Desktop\truncated_data\run.py", line 24
    return f"'{re.sub(r'\'', r'\'\'', value)}'"
                                               ^
SyntaxError: f-string expression part cannot include a backslash


come on, you cant keep returning me a faulty code...

ChatGPT
I apologize for the issues you're encountering. Let's correct the problem with the string handling in the script.

Instead of using an f-string for escaping single quotes in the process_value function, we will use a more straightforward approach to replace single quotes with doubled single quotes. This should fix the syntax error you're experiencing with the f-string.

Here's the revised script:

python
Copy code
import csv

# Define the mapping of TSV files to their corresponding table names and columns
file_table_mappings = {
    'truncated_name.basics.tsv': ('people', ['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles', 'img_url_asset']),
    'truncated_title.akas.tsv': ('title_akas', ['titleId', 'ordering', 'title', 'region', 'language', 'types', 'attributes', 'isOriginalTitle']),
    'truncated_title.basics.tsv': ('Titles', ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres', 'img_url_asset']),
    'truncated_title.crew.tsv': ('title_crew', ['tconst', 'directors', 'writers']),
    'truncated_title.episode.tsv': ('Episodes', ['tconst', 'parentTconst', 'seasonNumber', 'episodeNumber']),
    'truncated_title.principals.tsv': ('title_principals', ['tconst', 'ordering', 'nconst', 'category', 'job', 'characters', 'img_url_asset']),
    'truncated_title.ratings.tsv': ('Title_ratings', ['tconst', 'averageRating', 'numVotes'])
}

# Helper function to process each value
def process_value(value):
    if value == r'\N':
        return 'NULL'
    if value == '1':
        return 'TRUE'
    if value == '0':
        return 'FALSE'
    # Escape single quotes for SQL
    return "'" + value.replace("'", "''") + "'"

# Function to create SQL INSERT statements from TSV data
def create_insert_statements(filename, table, columns):
    insert_statements = []
    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file, delimiter='\t', fieldnames=columns)

        next(reader)  # Skip header row

        for row in reader:
            # Skip rows with missing key fields
            if r'\N' in [row[col] for col in ['tconst', 'nconst'] if col in row]:
                continue

            # Prepare values and create the INSERT statement
            values = [process_value(row[col]) for col in columns]
            insert_statement = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(values)});"
            insert_statements.append(insert_statement)

    return insert_statements

# Main process
sql_statements = []
for filename, (table, columns) in file_table_mappings.items():
    try:
        sql_statements += create_insert_statements(filename, table, columns)
    except Exception as e:
        print(f"Error processing {filename}: {e}")

# Write the SQL statements to a file
with open('database_inserts.sql', 'w', encoding='utf-8') as sql_file:
    sql_file.write('\n'.join(sql_statements))

print("SQL file generated successfully.")
In this version, I replaced the f-string in process_value with simple concatenation and the replace method to handle single quotes. This should resolve the syntax error and work correctly. Please run this revised script.




User
now the .sql is successfully created
there is a huge error that happens that i havent seen you check.

in the rows that have the column "ordering" thats just the # of the title thats being handled, so, it will always start from 1 up to the number of crews for example.
in your code, that number 1 you turn it into a TRUE value, but it should remain as 1.
the problem is that you shouldnt run process_value everywhere, just at the \n and bools

ChatGPT
I understand the issue. The script should only apply the process_value function to the \N values and the boolean columns, not to every value indiscriminately. We need to modify the script to handle this appropriately.

Let's adjust the script to only apply the conversion for \N values and boolean columns (isAdult and isOriginalTitle), and keep other values as they are, particularly the ordering column which should remain numeric:

python
Copy code
import csv

# Define the mapping of TSV files to their corresponding table names and columns
file_table_mappings = {
    'truncated_name.basics.tsv': ('people', ['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles', 'img_url_asset']),
    'truncated_title.akas.tsv': ('title_akas', ['titleId', 'ordering', 'title', 'region', 'language', 'types', 'attributes', 'isOriginalTitle']),
    'truncated_title.basics.tsv': ('Titles', ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres', 'img_url_asset']),
    'truncated_title.crew.tsv': ('title_crew', ['tconst', 'directors', 'writers']),
    'truncated_title.episode.tsv': ('Episodes', ['tconst', 'parentTconst', 'seasonNumber', 'episodeNumber']),
    'truncated_title.principals.tsv': ('title_principals', ['tconst', 'ordering', 'nconst', 'category', 'job', 'characters', 'img_url_asset']),
    'truncated_title.ratings.tsv': ('Title_ratings', ['tconst', 'averageRating', 'numVotes'])
}

# Function to process special values
def process_special_value(value, col_name):
    if value == r'\N':
        return 'NULL'
    if col_name in ['isAdult', 'isOriginalTitle']:
        return 'TRUE' if value == '1' else 'FALSE' if value == '0' else value
    # Escape single quotes for SQL
    return "'" + value.replace("'", "''") + "'"

# Function to create SQL INSERT statements from TSV data
def create_insert_statements(filename, table, columns):
    insert_statements = []
    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file, delimiter='\t', fieldnames=columns)

        next(reader)  # Skip header row

        for row in reader:
            # Skip rows with missing key fields
            if r'\N' in [row[col] for col in ['tconst', 'nconst'] if col in row]:
                continue

            # Prepare values and create the INSERT statement
            values = [process_special_value(row[col], col) for col in columns]
            insert_statement = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(values)});"
            insert_statements.append(insert_statement)

    return insert_statements

# Main process
sql_statements = []
for filename, (table, columns) in file_table_mappings.items():
    try:
        sql_statements += create_insert_statements(filename, table, columns)
    except Exception as e:
        print(f"Error processing {filename}: {e}")

# Write the SQL statements to a file
with open('database_inserts.sql', 'w', encoding='utf-8') as sql_file:
    sql_file.write('\n'.join(sql_statements))

print("SQL file generated successfully.")
In this version, the process_special_value function will only apply conversions for \N values and the specified boolean columns (isAdult and isOriginalTitle). It will leave other values, like ordering, in their original numeric form. Please try running this revised script.




User
good! everything is now running smoothly.
i had to make a few changes to the titles of each table or the input python script, like this:

DROP DATABASE IF EXISTS ntuaflix;

CREATE DATABASE ntuaflix;
USE ntuaflix;

CREATE TABLE users (
    userID INT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(255),
    email VARCHAR(255),
    password_hashed VARCHAR(255)
);

CREATE TABLE people (
    nconst VARCHAR(255) PRIMARY KEY,
    primaryName VARCHAR(255),
    birthYear VARCHAR(255),
    deathYear VARCHAR(255),
    primaryProfession VARCHAR(255),
    knownForTitles VARCHAR(255),
    img_url_asset VARCHAR(255)
);

CREATE TABLE Titles (
    tconst VARCHAR(255) PRIMARY KEY,
    titletype VARCHAR(255),
    primarytitle VARCHAR(255),
    originaltitle VARCHAR(255),
    isAdult BOOLEAN,
    startYear VARCHAR(255),
    endYear VARCHAR(255),
    runtimeMinutes VARCHAR(255),
    genres VARCHAR(255),
    img_url_asset VARCHAR(255)
);

CREATE TABLE Episodes (
    tconst VARCHAR(255) PRIMARY KEY,
    parentTconst VARCHAR(255),
    seasonN VARCHAR(255),
    episodeN VARCHAR(255),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE Title_ratings ( 
    titleid VARCHAR(255) PRIMARY KEY,
    averageRate DECIMAL(3, 2),
    numVotes INT,
    FOREIGN KEY (titleid) REFERENCES Titles(tconst)
);

CREATE TABLE title_crew (
    tconst VARCHAR(255),
    directors VARCHAR(255),
    writers VARCHAR(255),
    PRIMARY KEY (tconst),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE title_principals (
    tconst VARCHAR(255),
    ordering INT,
    nconst VARCHAR(255),
    category VARCHAR(255),
    job VARCHAR(255),
    characters VARCHAR(255),
    img_url_asset VARCHAR(255),
    PRIMARY KEY (tconst, ordering),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst),
    FOREIGN KEY (nconst) REFERENCES people(nconst)
);

CREATE TABLE title_akas (
    tconst VARCHAR(255),
    ordering INT,
    title VARCHAR(255),
    region VARCHAR(255),
    language VARCHAR(255),
    types VARCHAR(255),
    attributes VARCHAR(255),
    isOriginalTitle BOOLEAN,
    PRIMARY KEY (tconst, ordering),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE watchlist (
    userID INT,
    tconst VARCHAR(255),
    watched BOOLEAN,
    PRIMARY KEY (userID, tconst),
    FOREIGN KEY (userID) REFERENCES users(userID),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE forum (
    forumID INT AUTO_INCREMENT PRIMARY KEY,
    tconst VARCHAR(255),
    title VARCHAR(255),
    description VARCHAR(255),
    FOREIGN KEY (tconst) REFERENCES Titles(tconst)
);

CREATE TABLE comments (
    commentID INT AUTO_INCREMENT PRIMARY KEY,
    forumID INT,
    userID INT,
    commentText VARCHAR(255),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (forumID) REFERENCES forum(forumID),
    FOREIGN KEY (userID) REFERENCES users(userID)
);


import csv

# Define the mapping of TSV files to their corresponding table names and columns
file_table_mappings = {
    'truncated_name.basics.tsv': ('people', ['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles', 'img_url_asset']),
    'truncated_title.basics.tsv': ('Titles', ['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres', 'img_url_asset']),
    'truncated_title.episode.tsv': ('Episodes', ['tconst', 'parentTconst', 'seasonN', 'episodeN']),
    'truncated_title.ratings.tsv': ('Title_ratings', ['titleid', 'averageRate', 'numVotes']),
    'truncated_title.crew.tsv': ('title_crew', ['tconst', 'directors', 'writers']),
    'truncated_title.principals.tsv': ('title_principals', ['tconst', 'ordering', 'nconst', 'category', 'job', 'characters', 'img_url_asset']),
    'truncated_title.akas.tsv': ('title_akas', ['tconst', 'ordering', 'title', 'region', 'language', 'types', 'attributes', 'isOriginalTitle'])

}

# Function to process special values
def process_special_value(value, col_name):
    if value == r'\N':
        return 'NULL'
    if col_name in ['isAdult', 'isOriginalTitle']:
        return 'TRUE' if value == '1' else 'FALSE' if value == '0' else value
    # Escape single quotes for SQL
    return "'" + value.replace("'", "''") + "'"

# Function to create SQL INSERT statements from TSV data
def create_insert_statements(filename, table, columns):
    insert_statements = []
    with open(filename, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file, delimiter='\t', fieldnames=columns)

        next(reader)  # Skip header row

        for row in reader:
            # Skip rows with missing key fields
            if r'\N' in [row[col] for col in ['tconst', 'nconst'] if col in row]:
                continue

            # Prepare values and create the INSERT statement
            values = [process_special_value(row[col], col) for col in columns]
            insert_statement = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({', '.join(values)});"
            insert_statements.append(insert_statement)

    return insert_statements

# Main process
sql_statements = []
for filename, (table, columns) in file_table_mappings.items():
    try:
        sql_statements += create_insert_statements(filename, table, columns)
    except Exception as e:
        print(f"Error processing {filename}: {e}")

# Write the SQL statements to a file
with open('database_inserts.sql', 'w', encoding='utf-8') as sql_file:
    sql_file.write('\n'.join(sql_statements))

print("SQL file generated successfully.")

im giving you this input so you can better yourself

ChatGPT
Thank you for sharing the final adjustments you made. Your modifications to the script and the SQL table schema are well thought out and appropriate for the task at hand. It's great to hear that the script is now running smoothly.

The changes you made to the script, such as updating the file_table_mappings dictionary to match your database schema and refining the process_special_value function, are crucial steps in ensuring that the data from the .tsv files is correctly processed and inserted into your database.

Your feedback is highly valuable, and it helps in refining the approach to similar tasks in the future. If you have any more questions or need further assistance with your project, feel free to ask!




User
can you make it in the python script, when the .sql file already exists, to delete it and recreate it?

ChatGPT
Certainly! To modify the script so that it deletes the existing .sql file and recreates it, you can use Python's os module to check if the file exists and delete it if necessary before writing the new .sql file. Here's the updated section of the script with this functionality:

First, make sure to import the os module at the beginning of your script:

python
Copy code
import os
Then, update the part of the script where you write the SQL statements to a file:

python
Copy code
# Define the path for the SQL file
sql_file_path = 'database_inserts.sql'

# Check if the file already exists and delete it if it does
if os.path.exists(sql_file_path):
    os.remove(sql_file_path)

# Write the SQL statements to a new file
with open(sql_file_path, 'w', encoding='utf-8') as sql_file:
    sql_file.write('\n'.join(sql_statements))

print("SQL file generated successfully.")
With this update, the script will check for the existence of database_inserts.sql before writing. If the file already exists, it will be removed, and a new file will be created with the updated SQL statements. This ensures that every time you run the script, it starts fresh with a new .sql file.
